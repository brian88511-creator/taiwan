import streamlit as st
import os
from langchain_openai import OpenAIEmbeddings
from langchain_pinecone import PineconeVectorStore
from langchain_groq import ChatGroq
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder # <<< è®Šæ›´é»
from langchain_core.runnables import RunnablePassthrough # åŸå§‹ import (RunnablePassthrough)
from langchain_core.output_parsers import StrOutputParser
from langchain_core.messages import HumanMessage, AIMessage # <<< è®Šæ›´é»
from langchain.chains import create_history_aware_retriever, create_retrieval_chain # <<< è®Šæ›´é»
from langchain.chains.combine_documents import create_stuff_documents_chain # <<< è®Šæ›´é»

# --- 1. å¾ Streamlit Secrets è®€å–è¨­å®š ---
# (æ­¤éƒ¨åˆ†èˆ‡æ‚¨åŸå§‹ç¢¼ç›¸åŒï¼Œä¿æŒä¸è®Š)
try:
    os.environ["OPENAI_API_KEY"] = st.secrets["OPENAI_API_KEY"]
    os.environ["PINECONE_API_KEY"] = st.secrets["PINECONE_API_KEY"]
    os.environ["GROQ_API_KEY"] = st.secrets["GROQ_API_KEY"]

    PINECONE_INDEX_NAME = st.secrets["PINECONE_INDEX_NAME"]
    ACTIVE_GROQ_MODEL = st.secrets["GROQ_MODEL_NAME"]
    OPENAI_EMBED_MODEL = "text-embedding-3-small"
    
    all_keys_loaded = True

except KeyError:
    all_keys_loaded = False
    st.error("âŒ åš´é‡éŒ¯èª¤ï¼šç¼ºå°‘ API Keys æˆ–è¨­å®šã€‚è«‹åœ¨ Streamlit Cloud çš„ Secrets ä¸­è¨­å®šã€‚")
except Exception as e:
    all_keys_loaded = False
    st.error(f"âŒ ç™¼ç”Ÿæœªé æœŸçš„éŒ¯èª¤ï¼š{e}")


# --- 2. (é‡è¦ï¼) å¿«å– RAG éˆ (å·²å‡ç´šç‚ºå°è©±æ¨¡å¼) ---
# <<< è®Šæ›´é»ï¼šæ•´å€‹ get_rag_chain å‡½æ•¸å·²é‡æ§‹ >>>
@st.cache_resource
def get_rag_chain():
    print("... æ­£åœ¨åˆå§‹åŒ– RAG ç³»çµ± (å°è©±æ¨¡å¼)...")

    # 1. (Embeddings) åˆå§‹åŒ–ã€Œæ–‡å­—ç¿»è­¯å®˜ã€
    embeddings = OpenAIEmbeddings(model=OPENAI_EMBED_MODEL)

    # 2. (Vector Store) åˆå§‹åŒ– Pinecone çŸ¥è­˜åº«
    vectorstore = PineconeVectorStore.from_existing_index(
        index_name=PINECONE_INDEX_NAME,
        embedding=embeddings
    )

    # 3. (Retriever) å»ºç«‹ä¸€å€‹ã€Œæª¢ç´¢å™¨ã€
    retriever = vectorstore.as_retriever(search_kwargs={'k': 3}) # k=3: æ¯æ¬¡æŠ“ 3 ä»½ç›¸é—œè³‡æ–™

    # 4. (LLM) åˆå§‹åŒ– Llama 3 å¤§è…¦
    llm = ChatGroq(model_name=ACTIVE_GROQ_MODEL)

    # 5. (System Prompt - æ”¹å¯«) å»ºç«‹ã€Œæ”¹å¯«å•é¡Œã€çš„æç¤ºè©
    # é€™å€‹ Prompt å°ˆé–€ç”¨ä¾†å°‡æ–°å•é¡Œå’ŒèˆŠæ­·å²çµåˆï¼Œç”¢ç”Ÿä¸€å€‹ç¨ç«‹çš„å•é¡Œ
    contextualize_q_system_prompt = """
    è«‹æ ¹æ“šã€Œå°è©±æ­·å²ã€å’Œã€Œä½¿ç”¨è€…çš„æœ€æ–°å•é¡Œã€ï¼Œ
    å°‡ã€Œä½¿ç”¨è€…çš„æœ€æ–°å•é¡Œã€æ”¹å¯«æˆä¸€å€‹ã€Œç¨ç«‹ã€å®Œæ•´çš„å•é¡Œã€ã€‚
    é€™å€‹å•é¡Œå°‡è¢«ç”¨ä¾†æª¢ç´¢ç›¸é—œè³‡æ–™ã€‚
    å¦‚æœã€Œä½¿ç”¨è€…çš„æœ€æ–°å•é¡Œã€æœ¬èº«å·²ç¶“å¾ˆå®Œæ•´ï¼Œå°±ç›´æ¥å›å‚³å®ƒï¼Œä¸è¦ä¿®æ”¹ã€‚
    
    ã€å°è©±æ­·å²ã€‘:
    {chat_history}
    """
    
    contextualize_q_prompt = ChatPromptTemplate.from_messages(
        [
            ("system", contextualize_q_system_prompt),
            MessagesPlaceholder(variable_name="chat_history"), # æ”¾å…¥å°è©±æ­·å²
            ("human", "{input}"), # æ”¾å…¥ä½¿ç”¨è€…çš„æœ€æ–°å•é¡Œ
        ]
    )

    # 6. (Chain - æ”¹å¯«) å»ºç«‹ã€Œæ­·å²æ„ŸçŸ¥æª¢ç´¢å™¨ã€éˆ
    # é€™å€‹éˆæœƒ (1) æ¥æ”¶æ­·å²å’Œæ–°å•é¡Œ (2) å‘¼å« LLM ç”¢ç”Ÿæ–°å•é¡Œ (3) æ‹¿æ–°å•é¡Œå»æª¢ç´¢
    history_aware_retriever = create_history_aware_retriever(
        llm, retriever, contextualize_q_prompt
    )

    # 7. (System Prompt - å›ç­”) å»ºç«‹ã€Œå›ç­”å•é¡Œã€çš„æç¤ºè© (æ²¿ç”¨æ‚¨çš„äººè¨­)
    # é€™å€‹ Prompt æœƒæ¥æ”¶ã€Œå°è©±æ­·å²ã€ã€ã€Œæª¢ç´¢åˆ°çš„è³‡æ–™ã€å’Œã€Œæ–°å•é¡Œã€
    system_prompt = """
    ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„ã€Œå°ç£åœ¨åœ°æ–‡åŒ–ã€å°è¦½å°ˆå®¶èˆ‡ç ”ç©¶å­¸è€…ã€‚
    ä½ çš„ä»»å‹™æ˜¯æ ¹æ“šæˆ‘æä¾›çš„ã€Œåƒè€ƒè³‡æ–™ã€å’Œæˆ‘ä¹‹å‰çš„å°è©±ï¼Œä¾†ç²¾ç¢ºä¸”å°ˆæ¥­åœ°å›ç­”å•é¡Œã€‚

    - è«‹ã€Œåš´æ ¼ã€ä¾ç…§æˆ‘çµ¦çš„ã€Œåƒè€ƒè³‡æ–™ã€ä¾†å›ç­”ã€‚
    - å¦‚æœã€Œåƒè€ƒè³‡æ–™ã€ä¸­æ²’æœ‰æåˆ°ï¼Œè«‹èª å¯¦åœ°èªªï¼šã€Œæ ¹æ“šæˆ‘ç›®å‰æ“æœ‰çš„è³‡æ–™ï¼Œæˆ‘ç„¡æ³•å›ç­”é€™å€‹å•é¡Œã€‚ã€
    - ä½ çš„èªæ°£æ‡‰è©²æ˜¯è¦ªåˆ‡ã€æœ‰æ·±åº¦ä¸”å¯Œå«æ–‡åŒ–åº•è˜Šçš„ã€‚
    - è«‹ç”¨ç¹é«”ä¸­æ–‡å›ç­”ã€‚

    ã€åƒè€ƒè³‡æ–™ã€‘:
    {context}
    """
    
    qa_prompt = ChatPromptTemplate.from_messages(
        [
            ("system", system_prompt),
            MessagesPlaceholder(variable_name="chat_history"), # æ”¾å…¥å°è©±æ­·å²
            ("human", "{input}"), # æ”¾å…¥ä½¿ç”¨è€…çš„æœ€æ–°å•é¡Œ
        ]
    )
    
    # 8. (Chain - å›ç­”) å»ºç«‹ã€Œæ–‡ä»¶è™•ç†éˆã€
    # é€™å€‹éˆå°ˆé–€è² è²¬å°‡æª¢ç´¢åˆ°çš„æ–‡ä»¶(context)å¡é€² Prompt ä¸­
    question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)
    
    # 9. (Chain - ç¸½éˆ) å»ºç«‹ã€ŒRAG ç¸½éˆã€
    # é€™æ˜¯æˆ‘å€‘æœ€çµ‚è¦é‹è¡Œçš„éˆ
    # (1) å‘¼å« history_aware_retriever (å®ƒæœƒè‡ªå‹•æ”¹å¯«å•é¡Œä¸¦æª¢ç´¢)
    # (2) å°‡æª¢ç´¢çµæœå’ŒåŸå§‹è¼¸å…¥å‚³éçµ¦ question_answer_chain ä¾†ç”Ÿæˆç­”æ¡ˆ
    rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)
    
    print("ğŸ‰ RAG èŠå¤©æ©Ÿå™¨äºº (å°è©±æ¨¡å¼) å·²æº–å‚™å°±ç·’ï¼")
    return rag_chain

# --- 3. Streamlit èŠå¤©ä»‹é¢ ---

st.title("ğŸ‡¹ğŸ‡¼ å°ç£æ–‡åŒ–å°ˆå®¶")
st.caption("ä¸€å€‹åŸºæ–¼æ‚¨åœ¨åœ°æ–‡å²è³‡æ–™çš„ RAG ç³»çµ± (ä½¿ç”¨ Llama 3 ä¸¦å…·å‚™è¨˜æ†¶åŠŸèƒ½)")

# åªæœ‰åœ¨æ‰€æœ‰å¯†é‘°éƒ½è¼‰å…¥æˆåŠŸæ™‚ï¼Œæ‰åŸ·è¡Œ RAG éˆå’ŒèŠå¤©
if all_keys_loaded:

    # å–å¾—å¿«å–çš„ RAG éˆ
    try:
        rag_chain = get_rag_chain()

        # åˆå§‹åŒ–èŠå¤©è¨˜éŒ„ (å„²å­˜åœ¨ session_state)
        if "messages" not in st.session_state:
            st.session_state.messages = []

        # é¡¯ç¤ºéå»çš„èŠå¤©è¨˜éŒ„
        for message in st.session_state.messages:
            with st.chat_message(message["role"]):
                st.markdown(message["content"])

        # <<< è®Šæ›´é»ï¼šä¿®æ”¹ chat_input çš„æç¤ºæ–‡å­—ï¼Œä½¿å…¶èˆ‡äººè¨­ä¸€è‡´ >>>
        if user_question := st.chat_input("è«‹è¼¸å…¥é—œæ–¼å°ç£åœ¨åœ°æ–‡åŒ–çš„å•é¡Œ..."):
            # é¡¯ç¤ºç”¨æˆ¶è¨Šæ¯
            st.session_state.messages.append({"role": "user", "content": user_question})
            with st.chat_message("user"):
                st.markdown(user_question)

            # é¡¯ç¤º AI å›æ‡‰
            with st.chat_message("assistant"):
                with st.spinner("AI æ­£åœ¨æª¢ç´¢æ‚¨çš„è«–æ–‡è³‡æ–™åº«ä¸¦æ€è€ƒä¸­..."):

                    # <<< è®Šæ›´é»ï¼šæº–å‚™ LangChain éœ€è¦çš„å°è©±æ­·å²æ ¼å¼ >>>
                    # st.session_state.messages å„²å­˜çš„æ˜¯ dict
                    # LangChain çš„ MessagesPlaceholder éœ€è¦çš„æ˜¯ HumanMessage / AIMessage ç‰©ä»¶
                    chat_history = []
                    for msg in st.session_state.messages:
                        if msg["role"] == "user":
                            chat_history.append(HumanMessage(content=msg["content"]))
                        else:
                            chat_history.append(AIMessage(content=msg["content"]))

                    # <<< è®Šæ›´é»ï¼šå‘¼å« RAG éˆçš„æ–¹å¼æ”¹è®Šäº† >>>
                    # èˆŠçš„ï¼šresponse = rag_chain.invoke(user_question)
                    # æ–°çš„ï¼šæˆ‘å€‘å‚³å…¥ä¸€å€‹ dictï¼ŒåŒ…å« "input" å’Œ "chat_history"
                    response_dict = rag_chain.invoke(
                        {
                            "input": user_question,
                            "chat_history": chat_history 
                        }
                    )
                    
                    # <<< è®Šæ›´é»ï¼šå¾å›å‚³çš„ dict ä¸­æå–ç­”æ¡ˆ >>>
                    # create_retrieval_chain çš„å›å‚³æ˜¯ä¸€å€‹ dictï¼Œç­”æ¡ˆåœ¨ "answer" æ¬„ä½
                    response = response_dict["answer"]
                    
                    st.markdown(response)

            # å„²å­˜ AI å›æ‡‰
            st.session_state.messages.append({"role": "assistant", "content": response})

    except Exception as e:
        st.error(f"âŒ åŸ·è¡Œ RAG éˆæ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")
        st.error("è«‹æª¢æŸ¥æ‚¨çš„ Pinecone ç´¢å¼•åç¨±æˆ– API Keys æ˜¯å¦æ­£ç¢ºã€‚")

else:
    st.warning("App æœªèƒ½åˆå§‹åŒ–ï¼Œè«‹æª¢æŸ¥ Secrets è¨­å®šã€‚")
