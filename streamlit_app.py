import streamlit as st
import os
from langchain_openai import OpenAIEmbeddings
from langchain_pinecone import PineconeVectorStore
from langchain_groq import ChatGroq
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser

# --- 1. å¾ Streamlit Secrets è®€å–è¨­å®š ---
# 
# 
try:
    os.environ["OPENAI_API_KEY"] = st.secrets["OPENAI_API_KEY"]
    os.environ["PINECONE_API_KEY"] = st.secrets["PINECONE_API_KEY"]
    os.environ["GROQ_API_KEY"] = st.secrets["GROQ_API_KEY"]

    PINECONE_INDEX_NAME = st.secrets["PINECONE_INDEX_NAME"]
    ACTIVE_GROQ_MODEL = st.secrets["GROQ_MODEL_NAME"]
    OPENAI_EMBED_MODEL = "text-embedding-3-small"

    # æ¨™è¨˜ï¼šç¢ºèªæ‰€æœ‰å¯†é‘°éƒ½æˆåŠŸè®€å–
    all_keys_loaded = True

except KeyError:
    # å¦‚æœåœ¨æœ¬åœ°æ¸¬è©¦ï¼ˆæ²’æœ‰ st.secretsï¼‰ï¼Œé€™æœƒç™¼ç”Ÿ
    # ç‚ºäº†è®“ App è‡³å°‘èƒ½é¡¯ç¤ºéŒ¯èª¤ï¼Œæˆ‘å€‘æ•æ‰å®ƒ
    all_keys_loaded = False
    st.error("âŒ åš´é‡éŒ¯èª¤ï¼šç¼ºå°‘ API Keys æˆ–è¨­å®šã€‚è«‹åœ¨ Streamlit Cloud çš„ Secrets ä¸­è¨­å®šã€‚")
except Exception as e:
    all_keys_loaded = False
    st.error(f"âŒ ç™¼ç”Ÿæœªé æœŸçš„éŒ¯èª¤ï¼š{e}")


# --- 2. (é‡è¦ï¼) å¿«å– RAG éˆ ---
# ä½¿ç”¨ @st.cache_resource ç¢ºä¿æ¨¡å‹å’Œéˆåªè¢«è¼‰å…¥ä¸€æ¬¡
# é€™æ¨£ App æ‰æœ‰æ•ˆç‡ï¼Œä¸æœƒæ¯æ¬¡ç”¨æˆ¶æå•éƒ½é‡æ–°è¼‰å…¥
@st.cache_resource
def get_rag_chain():
    print("... æ­£åœ¨åˆå§‹åŒ– RAG ç³»çµ± (åªæœƒåŸ·è¡Œä¸€æ¬¡)...")

    # 1. (Embeddings) åˆå§‹åŒ–ã€Œæ–‡å­—ç¿»è­¯å®˜ã€
    embeddings = OpenAIEmbeddings(model=OPENAI_EMBED_MODEL)

    # 2. (Vector Store) åˆå§‹åŒ– Pinecone çŸ¥è­˜åº«
    vectorstore = PineconeVectorStore.from_existing_index(
        index_name=PINECONE_INDEX_NAME,
        embedding=embeddings
    )

    # 3. (Retriever) å»ºç«‹ä¸€å€‹ã€Œæª¢ç´¢å™¨ã€
    retriever = vectorstore.as_retriever(search_kwargs={'k': 3}) # k=3: æ¯æ¬¡æŠ“ 3 ä»½ç›¸é—œè³‡æ–™

    # 4. (LLM) åˆå§‹åŒ– Llama 3 å¤§è…¦
    llm = ChatGroq(model_name=ACTIVE_GROQ_MODEL)

    # 5. (System Prompt) å»ºç«‹ã€Œç³»çµ±æç¤ºè©ã€(ä½ çš„äººè¨­)
    system_prompt = """
    ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„ã€Œå°ç£åœ¨åœ°æ–‡åŒ–ã€å°è¦½å°ˆå®¶èˆ‡ç ”ç©¶å­¸è€…ã€‚
ä½ çš„ä»»å‹™æ˜¯åƒè€ƒæˆ‘æä¾›çš„ã€Œåƒè€ƒè³‡æ–™ã€çµåˆä½ åŸæœ¬çš„çŸ¥è­˜ä¾†å›ç­”å•é¡Œã€‚

- ä½ çš„èªæ°£æ‡‰è©²æ˜¯è¦ªåˆ‡ã€æœ‰æ·±åº¦ä¸”å¯Œå«æ–‡åŒ–åº•è˜Šçš„ã€‚
- è«‹ä¸»è¦æ ¹æ“šä½¿ç”¨è€…ä½¿ç”¨çš„èªè¨€å›ç­”ã€‚

    ã€åƒè€ƒè³‡æ–™ã€‘:
    {context}
    """

    # 6. (Chain) å»ºç«‹å®Œæ•´çš„ RAG è™•ç†éˆ
    prompt = ChatPromptTemplate.from_messages(
        [
            ("system", system_prompt),
            ("human", "{question}"),
        ]
    )

    rag_chain = (
        {"context": retriever, "question": RunnablePassthrough()}
        | prompt
        | llm
        | StrOutputParser()
    )
    print("ğŸ‰ RAG èŠå¤©æ©Ÿå™¨äººå·²æº–å‚™å°±ç·’ï¼")
    return rag_chain

# --- 3. Streamlit èŠå¤©ä»‹é¢ ---

st.title("ğŸ‡¹ğŸ‡¼ å°ç£æ–‡åŒ–å°ˆå®¶")
st.caption("ä¸€å€‹åŸºæ–¼æ‚¨åœ¨åœ°æ–‡å²è³‡æ–™çš„ RAG ç³»çµ± (ä½¿ç”¨ Llama 3)")

# åªæœ‰åœ¨æ‰€æœ‰å¯†é‘°éƒ½è¼‰å…¥æˆåŠŸæ™‚ï¼Œæ‰åŸ·è¡Œ RAG éˆå’ŒèŠå¤©
if all_keys_loaded:

    # å–å¾—å¿«å–çš„ RAG éˆ
    try:
        rag_chain = get_rag_chain()

        # åˆå§‹åŒ–èŠå¤©è¨˜éŒ„ (å„²å­˜åœ¨ session_state)
        if "messages" not in st.session_state:
            st.session_state.messages = []

        # é¡¯ç¤ºéå»çš„èŠå¤©è¨˜éŒ„
        for message in st.session_state.messages:
            with st.chat_message(message["role"]):
                st.markdown(message["content"])

        # è™•ç†æ–°çš„èŠå¤©è¼¸å…¥
        if user_question := st.chat_input("è«‹è¼¸å…¥é—œæ–¼å°ç£æ–‡åŒ–çš„å•é¡Œ..."):
            # é¡¯ç¤ºç”¨æˆ¶è¨Šæ¯
            st.session_state.messages.append({"role": "user", "content": user_question})
            with st.chat_message("user"):
                st.markdown(user_question)

            # é¡¯ç¤º AI å›æ‡‰
            with st.chat_message("assistant"):
                with st.spinner("AI æ­£åœ¨æª¢ç´¢æ‚¨çš„è«–æ–‡è³‡æ–™åº«ä¸¦æ€è€ƒä¸­..."):

                    # åŸ·è¡Œ RAG éˆ
                    response = rag_chain.invoke(user_question)
                    st.markdown(response)

            # å„²å­˜ AI å›æ‡‰
            st.session_state.messages.append({"role": "assistant", "content": response})

    except Exception as e:
        st.error(f"âŒ åŸ·è¡Œ RAG éˆæ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")
        st.error("è«‹æª¢æŸ¥æ‚¨çš„ Pinecone ç´¢å¼•åç¨±æˆ– API Keys æ˜¯å¦æ­£ç¢ºã€‚")

else:
    st.warning("App æœªèƒ½åˆå§‹åŒ–ï¼Œè«‹æª¢æŸ¥ Secrets è¨­å®šã€‚")
