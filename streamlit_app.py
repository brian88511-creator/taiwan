import streamlit as st
import os
from langchain_openai import OpenAIEmbeddings
from langchain_pinecone import PineconeVectorStore
from langchain_groq import ChatGroq
from langchain_core.prompts import ChatPromptTemplate
# ğŸŒŸ V-Memory å‡ç´šï¼šåŒ¯å…¥æ–°å·¥å…·
from langchain.chains.history_aware_retriever import create_history_aware_retriever
from langchain.chains.retrieval import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_core.prompts import MessagesPlaceholder # å°ˆé–€ç”¨ä¾†æ”¾ "èŠå¤©è¨˜éŒ„"
from langchain_core.messages import HumanMessage, AIMessage # ç”¨ä¾†è½‰æ›èŠå¤©è¨˜éŒ„çš„æ ¼å¼

# --- 1. å¾ Streamlit Secrets è®€å–è¨­å®š ---
# (é€™éƒ¨åˆ†å®Œå…¨ä¸è®Š)
try:
    os.environ["OPENAI_API_KEY"] = st.secrets["OPENAI_API_KEY"]
    os.environ["PINECONE_API_KEY"] = st.secrets["PINECONE_API_KEY"]
    os.environ["GROQ_API_KEY"] = st.secrets["GROQ_API_KEY"]
    
    PINECONE_INDEX_NAME = st.secrets["PINECONE_INDEX_NAME"]
    ACTIVE_GROQ_MODEL = st.secrets["GROQ_MODEL_NAME"]
    OPENAI_EMBED_MODEL = "text-embedding-3-small"
    
    all_keys_loaded = True
except KeyError:
    all_keys_loaded = False
    st.error("âŒ åš´é‡éŒ¯èª¤ï¼šç¼ºå°‘ API Keys æˆ–è¨­å®šã€‚è«‹åœ¨ Streamlit Cloud çš„ Secrets ä¸­è¨­å®šã€‚")
except Exception as e:
    all_keys_loaded = False
    st.error(f"âŒ ç™¼ç”Ÿæœªé æœŸçš„éŒ¯èª¤ï¼š{e}")


# --- 2. (é‡è¦ï¼) ğŸŒŸ V-Memory å‡ç´šï¼šå¿«å–ã€Œå°è©±å¼ RAG éˆã€ ---
# 
# 
@st.cache_resource
def get_conversational_rag_chain():
    print("... â›”ï¸ æ­£åœ¨åˆå§‹åŒ–ã€ŒV-Memory å°è©±å¼ RAG ç³»çµ±ã€(åªæœƒåŸ·è¡Œä¸€æ¬¡)...")
    
    # 1. (Embeddings) åˆå§‹åŒ–ã€Œæ–‡å­—ç¿»è­¯å®˜ã€ (ä¸è®Š)
    embeddings = OpenAIEmbeddings(model=OPENAI_EMBED_MODEL)

    # 2. (Vector Store) åˆå§‹åŒ– Pinecone çŸ¥è­˜åº« (ä¸è®Š)
    vectorstore = PineconeVectorStore.from_existing_index(
        index_name=PINECONE_INDEX_NAME,
        embedding=embeddings
    )

    # 3. (Retriever) å»ºç«‹ä¸€å€‹ã€Œæª¢ç´¢å™¨ã€ (ä¸è®Š)
    retriever = vectorstore.as_retriever(search_kwargs={'k': 3})

    # 4. (LLM) åˆå§‹åŒ– Llama 3 å¤§è…¦ (ä¸è®Š)
    llm = ChatGroq(model_name=ACTIVE_GROQ_MODEL)

    # --- ğŸŒŸ V-Memory å‡ç´šï¼šæˆ‘å€‘ç¾åœ¨éœ€è¦ã€Œå…©å€‹ã€ Prompt ---

    # 5. (Prompt 1) ã€Œå•é¡Œæ”¹å¯«ã€æç¤ºè© (Query Re-writing)
    # é€™æ˜¯ AI çš„ã€Œå…§éƒ¨å·¥ä½œã€ï¼Œç”¨ä¾†æŠŠã€Œé‚£...å‘¢ï¼Ÿã€æ”¹å¯«æˆã€Œè«‹ä»‹ç´¹...ã€
    contextualize_q_system_prompt = """
    ä½ æ˜¯ä¸€ä½æ”¹å¯«å•é¡Œçš„å°ˆå®¶ã€‚
    æ ¹æ“šã€ŒèŠå¤©è¨˜éŒ„ã€å’Œã€Œæœ€æ–°çš„ä½¿ç”¨è€…å•é¡Œã€ï¼Œ
    è«‹å°‡ã€Œæœ€æ–°çš„ä½¿ç”¨è€…å•é¡Œã€æ”¹å¯«æˆä¸€å€‹ã€Œç¨ç«‹ã€å®Œæ•´çš„å•é¡Œã€ï¼Œ
    é€™å€‹æ–°å•é¡Œå¿…é ˆèƒ½åœ¨ä¸çŸ¥é“èŠå¤©è¨˜éŒ„çš„æƒ…æ³ä¸‹è¢«ç¨ç«‹ç†è§£ã€‚
    
    - ã€Œä¸è¦ã€å›ç­”é€™å€‹å•é¡Œï¼Œä½ ã€Œåªè¦ã€æ”¹å¯«å®ƒã€‚
    - å¦‚æœã€Œæœ€æ–°çš„ä½¿ç”¨è€…å•é¡Œã€å·²ç¶“æ˜¯ç¨ç«‹å®Œæ•´çš„ï¼Œå°±ç›´æ¥å›å‚³å®ƒã€‚
    
    ã€èŠå¤©è¨˜éŒ„ã€‘:
    <chat_history>
    {chat_history}
    </chat_history>

    ã€æœ€æ–°çš„ä½¿ç”¨è€…å•é¡Œã€‘:
    {input}
    """
    
    contextualize_q_prompt = ChatPromptTemplate.from_messages(
        [
            ("system", contextualize_q_system_prompt),
            MessagesPlaceholder(variable_name="chat_history"), # æ”¾å…¥èŠå¤©è¨˜éŒ„
            ("human", "{input}"), # æ”¾å…¥ä½¿ç”¨è€…çš„æœ€æ–°å•é¡Œ
        ]
    )
    
    # 6. (Chain 1) å»ºç«‹ã€Œæ­·å²æ„ŸçŸ¥æª¢ç´¢å™¨ã€ (History-Aware Retriever)
    # é€™å€‹éˆæœƒ (1) æ¥æ”¶æ­·å²å’Œæ–°å•é¡Œ (2) åŸ·è¡Œ Prompt 1 (3) å¾—åˆ°æ”¹å¯«å¾Œçš„å•é¡Œ (4) ç”¨æ–°å•é¡Œå»æª¢ç´¢ Pinecone
    history_aware_retriever = create_history_aware_retriever(
        llm, retriever, contextualize_q_prompt
    )

    # 7. (Prompt 2) ã€Œæœ€çµ‚å›ç­”ã€æç¤ºè© (ä½ çš„äººè¨­)
    # ğŸŒŸ é€™å°±æ˜¯ä½ åŸæœ¬çš„ system_promptï¼Œä½†æˆ‘å€‘ç”¨æ–°æ–¹å¼ä¾†çµ„åˆ
    # ğŸŒŸ é€™è£¡çš„ {context} æœƒç”±ä¸Šé¢çš„ retriever æä¾›
    # ğŸŒŸ é€™è£¡çš„ {input} æ˜¯ä½¿ç”¨è€…ã€ŒåŸå§‹ã€çš„å•é¡Œ
    qa_system_prompt = """
    ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„å°ç£æ–‡åŒ–å°ˆå®¶ã€‚
    ä½ çš„ä»»å‹™æ˜¯æ ¹æ“šã€ŒèŠå¤©è¨˜éŒ„ã€å’Œã€Œåƒè€ƒè³‡æ–™ã€ä¾†ç²¾ç¢ºä¸”å°ˆæ¥­åœ°å›ç­”ã€Œæœ€æ–°çš„ä½¿ç”¨è€…å•é¡Œã€ã€‚

    - è«‹ã€Œåš´æ ¼ã€ä¾ç…§æˆ‘çµ¦çš„ã€Œåƒè€ƒè³‡æ–™ã€ä¾†å›ç­”ã€‚
    - å¦‚æœã€Œåƒè€ƒè³‡æ–™ã€ä¸­æ²’æœ‰æåˆ°ï¼Œè«‹èª å¯¦åœ°èªªï¼šã€Œæ ¹æ“šæˆ‘ç›®å‰æ“æœ‰çš„è³‡æ–™ï¼Œæˆ‘ç„¡æ³•å›ç­”é€™å€‹å•é¡Œã€‚ã€
    - ä½ çš„èªæ°£æ‡‰è©²æ˜¯è¦ªåˆ‡ã€æœ‰æ·±åº¦ä¸”å¯Œå«æ–‡åŒ–åº•è˜Šçš„ã€‚
    - è«‹ç”¨ç¹é«”ä¸­æ–‡å›ç­”ã€‚

    ã€åƒè€ƒè³‡æ–™ã€‘:
    {context}
    """

    qa_prompt = ChatPromptTemplate.from_messages(
        [
            ("system", qa_system_prompt),
            MessagesPlaceholder(variable_name="chat_history"), # æ”¾å…¥èŠå¤©è¨˜éŒ„
            ("human", "{input}"), # æ”¾å…¥ä½¿ç”¨è€…çš„æœ€æ–°å•é¡Œ
        ]
    )

    # 8. (Chain 2) å»ºç«‹ã€Œæ–‡ä»¶çµ„åˆéˆã€ (Stuff Documents Chain)
    # é€™å€‹éˆæœƒ (1) æ¥æ”¶æ‰€æœ‰æª¢ç´¢åˆ°çš„æ–‡ä»¶ (context) (2) æŠŠå®ƒå€‘ "stuff" (å¡) é€² Prompt 2
    question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)

    # 9. (Chain 3) å»ºç«‹ã€Œå°è©±å¼ RAG éˆã€ (Conversational RAG Chain)
    # é€™æ˜¯æœ€çµ‚çš„ç¸½éˆï¼
    # å®ƒæœƒè‡ªå‹•åŸ·è¡Œï¼š
    # 1. å‘¼å« Chain 1 (history_aware_retriever) -> å¾—åˆ°æ”¹å¯«çš„å•é¡Œ -> å¾—åˆ°æ–‡ä»¶ (context)
    # 2. å‘¼å« Chain 2 (question_answer_chain) -> å‚³å…¥ (context, chat_history, input) -> å¾—åˆ°æœ€çµ‚ç­”æ¡ˆ
    rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)

    print("ğŸ‰ V-Memory å°è©±å¼ RAG ç³»çµ±å·²æº–å‚™å°±ç·’ï¼")
    return rag_chain


# --- 3. Streamlit èŠå¤©ä»‹é¢ ---

# ğŸŒŸ V-Memory å‡ç´šï¼šè«‹åœ¨é€™è£¡ä¿®æ”¹ä½ çš„æ¨™é¡Œã€åœ–ç¤ºã€å´é‚Šæ¬„
st.set_page_config(page_title="å°ç£æ–‡åŒ–å°ˆå®¶", page_icon="ğŸ‡¹ğŸ‡¼") # ğŸš¨ (è«‹ä¿®æ”¹)
st.title("ğŸ‡¹ğŸ‡¼ å°ç£æ–‡åŒ–å°ˆå®¶") # ğŸš¨ (è«‹ä¿®æ”¹)
st.caption("ä¸€å€‹å…·å‚™ä¸Šä¸‹æ–‡è¨˜æ†¶çš„ RAG ç³»çµ± (ä½¿ç”¨ Llama 3)")

# ç¯„ä¾‹å´é‚Šæ¬„ (é¸å¡«)
with st.sidebar:
    st.header("ğŸ“– é—œæ–¼é€™å€‹ Demo")
    st.info(
        "é€™æ˜¯ä¸€å€‹ä½¿ç”¨ Conversational RAG æŠ€è¡“çš„ AI åŠ©ç†ã€‚\n"
        "å®ƒèƒ½è¨˜ä½æ‚¨ä¹‹å‰çš„å°è©±ï¼Œä¸¦ç†è§£ä¸Šä¸‹æ–‡ã€‚\n"
        "çŸ¥è­˜åº«åŒ…å«ï¼š\n"
        "- [ä½ çš„è«–æ–‡æ¨™é¡Œ 1]\n"
        "- [ä½ çš„è«–æ–‡æ¨™é¡Œ 2]\n"
    )

# åªæœ‰åœ¨æ‰€æœ‰å¯†é‘°éƒ½è¼‰å…¥æˆåŠŸæ™‚ï¼Œæ‰åŸ·è¡Œ
if all_keys_loaded:
    
    # å–å¾—å¿«å–çš„ RAG éˆ
    try:
        conversational_rag_chain = get_conversational_rag_chain()

        # åˆå§‹åŒ–èŠå¤©è¨˜éŒ„ (ä¸è®Š)
        if "messages" not in st.session_state:
            st.session_state.messages = []

        # é¡¯ç¤ºéå»çš„èŠå¤©è¨˜éŒ„ (ä¸è®Š)
        for message in st.session_state.messages:
            with st.chat_message(message["role"]):
                st.markdown(message["content"])

        # è™•ç†æ–°çš„èŠå¤©è¼¸å…¥
        if user_question := st.chat_input("è«‹è¼¸å…¥æ‚¨çš„å•é¡Œ..."):
            
            # é¡¯ç¤ºç”¨æˆ¶è¨Šæ¯ (ä¸è®Š)
            st.session_state.messages.append({"role": "user", "content": user_question})
            with st.chat_message("user"):
                st.markdown(user_question)

            # é¡¯ç¤º AI å›æ‡‰
            with st.chat_message("assistant"):
                with st.spinner("AI æ­£åœ¨æª¢ç´¢ä¸¦æ€è€ƒä¸­..."):
                    
                    # ğŸŒŸ V-Memory å‡ç´šï¼šæº–å‚™èŠå¤©è¨˜éŒ„ (è½‰æ›æ ¼å¼)
                    # æˆ‘å€‘åªå‚³é€æœ€å¾Œ 6 å‰‡è¨Šæ¯ (3è¼ªå°è©±) ä½œç‚ºè¨˜æ†¶ï¼Œé¿å… Token çˆ†æ»¿
                    chat_history_for_chain = []
                    for msg in st.session_state.messages[-6:]: # â¬…ï¸ åªå–æœ€å¾Œ 6 å‰‡
                        if msg["role"] == "user":
                            chat_history_for_chain.append(HumanMessage(content=msg["content"]))
                        elif msg["role"] == "assistant":
                            chat_history_for_chain.append(AIMessage(content=msg["content"]))
                    
                    # ğŸŒŸ V-Memory å‡ç´šï¼šå‘¼å« RAG éˆ
                    # èˆŠç‰ˆ: rag_chain.invoke(user_question)
                    # æ–°ç‰ˆ: å¿…é ˆå‚³å…¥ä¸€å€‹åŒ…å« "input" å’Œ "chat_history" çš„å­—å…¸
                    response = conversational_rag_chain.invoke({
                        "input": user_question,
                        "chat_history": chat_history_for_chain
                    })
                    
                    # ğŸŒŸ V-Memory å‡ç´šï¼šè§£æå›æ‡‰
                    # èˆŠç‰ˆ: response æ˜¯ä¸€å€‹å­—ä¸²
                    # æ–°ç‰ˆ: response æ˜¯ä¸€å€‹å­—å…¸ï¼Œç­”æ¡ˆåœ¨ "answer" éµè£¡é¢
                    answer = response["answer"]
                    
                    st.markdown(answer)
            
            # å„²å­˜ AI å›æ‡‰ (ä¸è®Š)
            st.session_state.messages.append({"role": "assistant", "content": answer})

    except Exception as e:
        st.error(f"âŒ åŸ·è¡Œ RAG éˆæ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")
        st.error("è«‹æª¢æŸ¥æ‚¨çš„ Pinecone ç´¢å¼•åç¨±æˆ– API Keys æ˜¯å¦æ­£ç¢ºã€‚")
        
else:
    st.warning("App æœªèƒ½åˆå§‹åŒ–ï¼Œè«‹æª¢æŸ¥ Secrets è¨­å®šã€‚")
